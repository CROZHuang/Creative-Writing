{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9kZeqZzbefs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_n21ceyc-esv",
        "outputId": "14d75351-990b-40aa-e459-f7989522355f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxiGXPkF_58x",
        "outputId": "5a59d7ca-f32b-4942-d6cf-ac9f6fa7cbd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (2.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfKBBroX_qnx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Creativity/Qwen2.5_7b_16_FT_WritingPrompts_full.xlsx'\n",
        "df = pd.read_excel(file_path, sheet_name ='Sheet1', engine = 'openpyxl')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "1XTM5D_6MLta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wNMOwdwi-qf",
        "outputId": "741b7f32-b703-481f-ae4c-7090d925e82b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EItWDatGjDDs"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJVWFPu1AGeS"
      },
      "outputs": [],
      "source": [
        "# Define columns for prompts and responses\n",
        "prompts = df['Prompts']\n",
        "response_1 = df['Response_1']\n",
        "response_2 = df['Response_2']\n",
        "response_3 = df['Response_3']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "an66_KsBjZZI"
      },
      "outputs": [],
      "source": [
        "def keyword_match(question, answer):\n",
        "    question_tokens = set(word_tokenize(question.lower())) - set(stopwords.words('english'))\n",
        "    answer_tokens = set(word_tokenize(answer.lower())) - set(stopwords.words('english'))\n",
        "    overlap = question_tokens & answer_tokens\n",
        "    return len(overlap) / len(question_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlSBHdmpAHCH"
      },
      "outputs": [],
      "source": [
        "# Initialize a list to store average similarity scores and detect functionality of responses\n",
        "Functionalities = []\n",
        "# Iterate through each prompt and its three responses\n",
        "for prompt, resp1, resp2, resp3 in zip(prompts, response_1, response_2, response_3):\n",
        "    tokens_p = prompt.split()\n",
        "    resp1 = str(resp1)\n",
        "    resp2 = str(resp2)\n",
        "    resp3 = str(resp3)\n",
        "    tokens_1 = resp1.split()\n",
        "    tokens_2 = resp2.split()\n",
        "    tokens_3 = resp3.split()\n",
        "\n",
        "    total_tokens_p = len(tokens_p)\n",
        "    unique_tokens_1 = len(set(tokens_1))\n",
        "    total_tokens_1 = len(tokens_1)\n",
        "    unique_tokens_2 = len(set(tokens_2))\n",
        "    total_tokens_2 = len(tokens_2)\n",
        "    unique_tokens_3 = len(set(tokens_3))\n",
        "    total_tokens_3 = len(tokens_3)\n",
        "    lexical_diversity_1 = unique_tokens_1 / total_tokens_1\n",
        "    lexical_diversity_2 = unique_tokens_2 / total_tokens_2\n",
        "    lexical_diversity_3 = unique_tokens_3 / total_tokens_3\n",
        "\n",
        "    # Dependency parsing for syntactic complexity\n",
        "    doc_1 = nlp(resp1)\n",
        "    dependency_count_1 = len([token for token in doc_1 if token.dep_ != \"ROOT\"])\n",
        "    doc_2 = nlp(resp2)\n",
        "    dependency_count_2 = len([token for token in doc_2 if token.dep_ != \"ROOT\"])\n",
        "    doc_3 = nlp(resp3)\n",
        "    dependency_count_3 = len([token for token in doc_3 if token.dep_ != \"ROOT\"])\n",
        "\n",
        "    match_1 = keyword_match(prompt, resp1)\n",
        "    match_2 = keyword_match(prompt, resp2)\n",
        "    match_3 = keyword_match(prompt, resp3)\n",
        "\n",
        "    func = 0\n",
        "\n",
        "    if (match_1 >= 0.10) and (dependency_count_1 >= 150) and (lexical_diversity_1 >= 0.5):\n",
        "      func += 1\n",
        "    if (match_2 >= 0.10) and (dependency_count_2 >= 150) and (lexical_diversity_2 >= 0.5):\n",
        "      func += 1\n",
        "    if (match_3 >= 0.10) and (dependency_count_3 >= 150) and (lexical_diversity_3 >= 0.5):\n",
        "      func += 1\n",
        "\n",
        "    # Calculate the average similarity and functionality\n",
        "    Functionalities.append(func)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2arDsforqdQ"
      },
      "outputs": [],
      "source": [
        "df['Functionality'] = Functionalities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qZytXXuATuz"
      },
      "outputs": [],
      "source": [
        "# Save the updated DataFrame back to Google Drive\n",
        "updated_file_path = '/content/drive/MyDrive/Creativity/Qwen2.5_7b_16_FT_WritingPrompts_full.xlsx'\n",
        "df.to_excel(updated_file_path, index=False, engine='openpyxl')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}